{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liamc\\anaconda3\\envs\\fencing_v3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch \n",
    "import pickle as pkl\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    label_list, data_list = [], []\n",
    "    for (data, label) in batch:\n",
    "\n",
    "        label_list.append(label)\n",
    "        data_list.append(data.squeeze())\n",
    "    \n",
    "    data_len = [d.shape[0] for d in data_list]\n",
    "    data_list = pad_sequence(data_list, batch_first=True, padding_value=0)\n",
    "    return (data_list, torch.Tensor(data_len)), torch.Tensor(label_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add N_Shot Feature\n",
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, annotation_file, pose_dir, ids_file, subset=None, n_shot=np.inf):\n",
    "            #[(name:side:start_frame:end_frame, action_label)]\n",
    "            #TODO: Implemenet N Shot Format\n",
    "            if subset is None:\n",
    "                self.subset = {line.split(' ', 1)[1].replace('\\n', '') for line in open(annotation_file, 'r').readlines()}\n",
    "            else: \n",
    "                self.subset = subset\n",
    "            #if subset is not None:\n",
    "            self.ids = [line.replace('\\n', '') for line in open(ids_file, 'r').readlines()]\n",
    "            self.counter = {action:0 for action in self.subset}\n",
    "            \n",
    "            self.label_dict = {lab:i for lab, i in zip(self.subset, range(len(self.subset)))}\n",
    "            self.pose_labels = []\n",
    "            for line in open(annotation_file, 'r'):\n",
    "                name, action = line.split(' ',1)\n",
    "                action = action.replace('\\n', '')\n",
    "                if action in self.subset and name in self.ids:\n",
    "                    if self.counter[action] < n_shot:\n",
    "                        self.counter[action] += 1\n",
    "                        self.pose_labels.append((name, action))\n",
    "\n",
    "            print(f'Action Item Counts {self.counter}')\n",
    "            self.pose_dir = pose_dir\n",
    "            self.n_shot = n_shot\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pose_labels)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        basename, side, start_frame, end_frame = self.pose_labels[idx][0].split(':')\n",
    "        pose_path = os.path.join(self.pose_dir,''.join([basename, '__', side, '.emb.pkl']))\n",
    "        with open(pose_path, 'rb') as f:\n",
    "            pose_emb =  pkl.load(f)\n",
    "        f.close()\n",
    "        poses = torch.Tensor([tup[1].reshape(-1, 128) for tup in pose_emb[int(start_frame) +1:int(end_frame) + 1]])\n",
    "        label = self.label_dict[self.pose_labels[idx][1]]\n",
    "        \n",
    "        return poses, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Item Counts {'advancing': 8, 'retreating': 6, 'fleche': 2, 'lunge': 7}\n"
     ]
    }
   ],
   "source": [
    "ann_file = 'C:\\\\Users\\\\liamc\\\\Desktop\\\\fencing_vision\\\\src\\\\vpd_fencing\\\\action_dataset\\\\fencing\\\\all.txt'\n",
    "pose_path = 'C:\\\\Users\\\\liamc\\\\Desktop\\\\fencing_vision\\\\data\\\\embeddings\\\\vivpd_res'\n",
    "val_ids = 'C:\\\\Users\\\\liamc\\\\Desktop\\\\fencing_vision\\\\src\\\\vpd_fencing\\\\action_dataset\\\\fencing\\\\val.ids.txt'\n",
    "dataset = PoseDataset(ann_file, pose_path, val_ids, subset = ['advancing', 'retreating', 'fleche', 'lunge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GREEN_BIDA_RUS_vs_BERTA_HUN__22:right:24:32', 'advancing'),\n",
       " ('GREEN_BIDA_RUS_vs_BERTA_HUN__17:left:14:29', 'retreating'),\n",
       " ('GREEN_BIDA_RUS_vs_BERTA_HUN__14:left:24:34', 'retreating'),\n",
       " ('GREEN_BIDA_RUS_vs_BERTA_HUN__6:right:29:36', 'lunge'),\n",
       " ('GREEN_BIDA_RUS_vs_BERTA_HUN__2:left:31:39', 'retreating'),\n",
       " ('BLUE_ALEXANIN_KAZ_vs_BOREL_FRA__15:left:26:35', 'lunge'),\n",
       " ('BLUE_ALEXANIN_KAZ_vs_BOREL_FRA__12:right:22:40', 'advancing'),\n",
       " ('BLUE_ALEXANIN_KAZ_vs_BOREL_FRA__10:left:39:46', 'fleche'),\n",
       " ('BLUE_ALEXANIN_KAZ_vs_BOREL_FRA__8:right:13:20', 'lunge'),\n",
       " ('BLUE_ALEXANIN_KAZ_vs_BOREL_FRA__5:right:0:25', 'advancing'),\n",
       " ('BLUE_ALEXANIN_KAZ_vs_BOREL_FRA__4:right:0:14', 'retreating'),\n",
       " ('BLUE_ALEXANIN_KAZ_vs_BOREL_FRA__4:left:29:37', 'advancing'),\n",
       " ('BARDENET_FRA_vs_CANNONE_FRA__4:left:20:28', 'advancing'),\n",
       " ('BARDENET_FRA_vs_CANNONE_FRA__3:left:23:32', 'lunge'),\n",
       " ('BARDENET_FRA_vs_LIMARDO_GASCON_VEN__8:right:38:50', 'lunge'),\n",
       " ('BARDENET_FRA_vs_LIMARDO_GASCON_VEN__2:left:6:31', 'advancing'),\n",
       " ('BARDENET_FRA_vs_LIMARDO_GASCON_VEN__1:right:0:17', 'retreating'),\n",
       " ('BARDENET_FRA_vs_LIMARDO_GASCON_VEN__1:left:0:28', 'advancing'),\n",
       " ('BARDENET_FRA_vs_CANNONE_FRA__17:right:11:21', 'fleche'),\n",
       " ('BARDENET_FRA_vs_CANNONE_FRA__12:right:0:7', 'advancing'),\n",
       " ('BARDENET_FRA_vs_CANNONE_FRA__11:right:32:43', 'lunge'),\n",
       " ('BARDENET_FRA_vs_CANNONE_FRA__10:right:37:44', 'lunge'),\n",
       " ('BARDENET_FRA_vs_CANNONE_FRA__9:right:0:17', 'retreating')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.pose_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.1810e-01, -1.8150e-01, -5.1929e-01,  4.4498e-01, -3.8933e-01,\n",
       "            1.1003e-01,  6.6490e-01,  4.3513e-02,  1.4381e+00, -3.6151e-01,\n",
       "           -1.0721e+00, -3.4841e-01, -7.3302e-03,  3.8814e-01,  3.5094e-01,\n",
       "           -1.6550e-04,  4.4455e-01,  1.2414e+00,  1.2185e-02, -2.6826e-01,\n",
       "            3.1255e-01,  3.0913e-01, -3.6451e-01, -8.4437e-02, -1.2006e-01,\n",
       "            1.9031e-01,  2.4545e-01,  6.6353e-02, -6.3859e-01,  5.1915e-01,\n",
       "           -4.6797e-01,  1.1918e+00, -3.1145e-01, -8.1848e-01,  8.8264e-01,\n",
       "           -2.2600e-02,  5.8654e-03, -4.0354e-03,  6.8581e-02, -7.4934e-01,\n",
       "            1.2965e-02,  1.3655e+00,  3.3225e-01, -3.1337e-01, -1.0402e-01,\n",
       "           -1.3228e-01,  6.4482e-01,  1.2166e-01, -1.9875e-01, -6.2479e-02,\n",
       "            1.3898e-01,  3.1026e-01,  5.9983e-01, -5.4693e-01, -5.2268e-01,\n",
       "            4.7994e-01, -1.7369e-01,  6.1162e-02, -3.0826e-01,  1.3331e-02,\n",
       "           -2.9080e-01,  3.1978e-01, -2.4767e-01,  6.7538e-01, -4.6452e-03,\n",
       "            1.0309e-01, -8.2404e-02, -3.0485e-01,  6.5423e-02, -5.1387e-02,\n",
       "           -2.3258e-01,  2.1001e-01, -3.4224e-01, -3.2100e-01, -5.1509e-01,\n",
       "            2.5341e-01, -3.8285e-01, -9.9893e-04,  5.7260e-01,  2.5922e-01,\n",
       "           -3.7075e-01,  1.0591e+00,  1.5327e-02, -1.9676e-01,  3.0848e-01,\n",
       "           -2.1225e+00,  1.2582e-01,  1.3992e-01, -2.7005e-01, -5.5906e-01,\n",
       "           -3.7224e-01, -8.5375e-01, -1.5322e-02,  7.9048e-01, -2.0946e-01,\n",
       "            2.1055e+00, -3.0093e-01,  7.5572e-01,  2.8403e-01, -2.8646e-01,\n",
       "            2.7795e-01,  2.5286e-01,  6.3283e-01, -3.9343e-01,  2.3967e-01,\n",
       "            8.7491e-01,  1.5748e-01, -8.6972e-02, -4.2221e-01, -4.9868e-01,\n",
       "            5.0768e-01, -2.0038e-01,  2.2253e-01,  2.1804e-01, -5.1700e-01,\n",
       "           -1.5433e-01,  3.8992e-01,  6.0956e-02, -2.5770e-01, -2.4842e-01,\n",
       "            1.1863e-01,  1.5899e-01,  2.1274e-01, -1.5710e-01,  1.1477e+00,\n",
       "            1.3588e-02, -3.4387e-01, -1.8293e-01]],\n",
       " \n",
       "         [[ 2.6326e-01, -1.8134e-01, -5.3032e-01,  4.1075e-01, -4.0195e-01,\n",
       "           -1.1546e-02,  1.0150e+00, -3.4229e-01,  1.2546e+00, -2.9719e-01,\n",
       "           -1.1891e+00, -4.3344e-01, -3.8648e-02,  4.1957e-01,  5.0744e-01,\n",
       "            1.0209e-01, -1.1652e-01,  1.4619e+00,  2.9259e-02, -4.5564e-01,\n",
       "            5.1147e-01,  3.3498e-01, -3.6104e-01, -1.2959e-01, -3.6754e-01,\n",
       "            2.7936e-01,  3.7849e-01,  2.7765e-01, -8.5929e-01,  5.1730e-01,\n",
       "           -5.8373e-01,  1.2057e+00, -4.2937e-01, -4.2317e-01,  9.1703e-01,\n",
       "           -3.8587e-03, -1.6671e-01,  1.7554e-01,  1.5048e-01, -6.3895e-01,\n",
       "            1.3311e-02,  1.4351e+00,  3.0100e-01, -9.1224e-02, -2.5029e-01,\n",
       "           -2.1816e-01,  5.9062e-01, -1.2287e-02, -3.0932e-01, -1.5393e-01,\n",
       "            1.8588e-01,  9.7633e-03,  5.9519e-01, -6.0509e-01, -5.0980e-01,\n",
       "            7.0442e-01, -1.9425e-01,  4.0402e-01, -1.5369e-01, -1.0694e-01,\n",
       "           -1.1456e-01,  2.8266e-01, -1.5399e-02,  9.1304e-01,  6.2101e-02,\n",
       "            3.6721e-02, -9.2784e-02, -2.5591e-01, -8.1750e-02, -1.7295e-01,\n",
       "            8.0077e-03,  7.0158e-02, -2.9066e-02, -3.4165e-01, -1.0616e+00,\n",
       "            1.7475e-01, -5.6040e-01,  7.9824e-02,  5.5907e-01,  4.9180e-01,\n",
       "           -7.1712e-01,  1.1164e+00, -4.9992e-02, -4.1336e-01,  4.2093e-01,\n",
       "           -2.2559e+00,  2.9895e-01,  2.3473e-02, -3.7795e-01, -4.8269e-01,\n",
       "           -1.6819e-01, -7.5492e-01, -1.3294e-01,  1.0007e+00, -2.8951e-01,\n",
       "            2.1775e+00, -6.9123e-01,  8.5205e-01,  2.4961e-01, -3.2966e-01,\n",
       "            3.7995e-01,  2.4889e-01,  5.1211e-01, -1.6811e-01,  2.1559e-01,\n",
       "            8.7477e-01,  1.9539e-01,  6.2206e-02, -4.9250e-01, -6.4458e-01,\n",
       "            4.5821e-01, -3.4282e-01,  3.1094e-01,  1.9194e-01, -3.8432e-01,\n",
       "           -2.5025e-01,  4.1421e-01,  1.4685e-01, -1.1753e-01, -1.9395e-01,\n",
       "            1.0618e-01,  1.8714e-01,  3.0125e-01, -2.2775e-01,  8.7264e-01,\n",
       "           -1.2445e-01, -3.6795e-01, -1.6265e-01]],\n",
       " \n",
       "         [[ 2.1413e-01, -1.8068e-01, -4.2937e-01, -2.6465e-03, -2.5713e-01,\n",
       "           -5.1821e-02,  1.3313e+00, -6.5733e-01,  1.0279e+00, -1.0697e-01,\n",
       "           -1.3573e+00, -1.6528e-01, -5.0953e-01,  7.1428e-01,  8.9428e-01,\n",
       "            2.3696e-01, -1.0551e+00,  1.5438e+00, -3.9950e-02, -4.0894e-01,\n",
       "            2.9833e-01, -1.6984e-01, -1.7019e-01, -3.4644e-01, -5.8666e-01,\n",
       "            2.7123e-01,  3.4917e-01,  4.6510e-01, -8.7349e-01,  4.1732e-01,\n",
       "           -9.0793e-01,  1.3952e+00, -8.0942e-01, -2.5446e-01,  7.3741e-01,\n",
       "            8.6258e-02,  2.2467e-02,  1.4589e-01,  3.1131e-01, -7.3029e-01,\n",
       "           -8.9728e-02,  1.1120e+00,  6.5566e-02,  2.0010e-01, -4.8958e-01,\n",
       "           -8.0945e-02,  7.9705e-01, -3.3023e-01, -5.9321e-01, -2.9113e-01,\n",
       "           -1.1817e-01, -4.3904e-02,  5.1598e-01, -8.0389e-01, -4.9444e-01,\n",
       "            1.0327e+00,  5.7921e-02,  6.6427e-01,  1.8804e-01, -4.5609e-01,\n",
       "           -6.3467e-01,  2.4567e-01,  3.6794e-01,  8.5280e-01,  2.6131e-01,\n",
       "            6.4277e-02, -1.8699e-01, -5.0769e-01, -1.9120e-01, -3.8811e-01,\n",
       "            5.1166e-01, -1.1783e-01,  3.8582e-01, -3.8538e-01, -1.7883e+00,\n",
       "            2.1504e-01, -7.3203e-01,  2.8696e-01,  5.9476e-01,  6.5965e-01,\n",
       "           -1.4210e+00,  1.1286e+00, -9.7107e-02, -6.9861e-01,  6.9654e-01,\n",
       "           -2.0070e+00,  6.1151e-02, -7.0682e-02, -5.1411e-01, -1.3067e-01,\n",
       "            1.7444e-03, -4.3241e-01, -2.8482e-01,  9.8398e-01,  4.6603e-02,\n",
       "            1.6913e+00, -9.4380e-01,  1.0138e+00, -2.0285e-02, -3.1251e-01,\n",
       "            4.7991e-01,  2.7604e-01,  2.3276e-01,  4.5782e-02,  1.5026e-01,\n",
       "            5.8218e-01,  1.6622e-01,  2.8061e-01, -4.8787e-01, -7.8330e-01,\n",
       "            3.5685e-01, -4.9263e-01,  5.1450e-01,  2.0438e-01, -4.0335e-01,\n",
       "           -4.4025e-01,  4.1776e-01,  1.7358e-01, -1.9233e-01,  1.9583e-01,\n",
       "            9.0708e-02,  1.9028e-01,  5.0039e-01, -2.9212e-01,  4.2598e-01,\n",
       "           -5.6377e-01, -1.2217e-01, -2.4641e-01]],\n",
       " \n",
       "         [[ 2.2842e-01, -1.7066e-01, -3.0245e-01, -3.9968e-01, -1.7030e-01,\n",
       "           -1.4921e-01,  1.3956e+00, -5.4023e-01,  1.0391e+00, -5.3722e-02,\n",
       "           -1.5035e+00, -1.1412e-01, -6.7814e-01,  7.5332e-01,  1.0472e+00,\n",
       "            2.3522e-01, -1.5812e+00,  1.5349e+00, -4.4999e-02, -2.8762e-01,\n",
       "            1.4329e-01, -4.4398e-01, -1.0587e-01, -6.2949e-01, -8.0083e-01,\n",
       "            2.7145e-01,  1.8380e-01,  4.4287e-01, -7.1423e-01,  3.4164e-01,\n",
       "           -8.1441e-01,  1.3968e+00, -1.0353e+00, -3.0906e-01,  6.3288e-01,\n",
       "            5.2374e-02,  2.7809e-01, -1.5186e-01,  3.6715e-01, -8.9395e-01,\n",
       "           -1.8515e-01,  1.0265e+00, -6.2388e-02,  1.6863e-01, -5.1150e-01,\n",
       "           -7.8338e-02,  1.0299e+00, -3.6066e-01, -5.7192e-01, -2.6383e-01,\n",
       "           -3.5991e-01, -1.1073e-01,  5.9513e-01, -7.7021e-01, -7.1430e-01,\n",
       "            1.1916e+00,  1.0239e-01,  5.7807e-01,  3.7507e-01, -6.2482e-01,\n",
       "           -9.4106e-01,  3.4374e-01,  4.8355e-01,  7.4476e-01,  2.4777e-01,\n",
       "            1.1428e-01, -2.5462e-01, -6.0544e-01, -1.9484e-01, -4.5885e-01,\n",
       "            6.8119e-01, -1.8043e-01,  6.3162e-01, -3.5330e-01, -2.0032e+00,\n",
       "            2.4942e-01, -7.0518e-01,  4.7224e-01,  7.3416e-01,  5.4060e-01,\n",
       "           -1.7810e+00,  1.1166e+00, -1.2072e-01, -6.5908e-01,  7.0616e-01,\n",
       "           -1.8123e+00, -1.1444e-01, -1.0565e-01, -6.7150e-01, -4.1041e-02,\n",
       "           -9.7425e-02, -2.5657e-01, -2.9152e-01,  8.9087e-01,  2.7873e-01,\n",
       "            1.4412e+00, -9.4917e-01,  8.4650e-01, -9.3989e-02, -1.9040e-01,\n",
       "            5.1072e-01,  2.6680e-01,  8.1835e-02,  7.7166e-02,  9.1402e-02,\n",
       "            4.5123e-01,  2.1871e-01,  2.4436e-01, -4.6045e-01, -8.2303e-01,\n",
       "            3.9391e-01, -4.5622e-01,  5.5761e-01,  1.6884e-01, -5.5919e-01,\n",
       "           -4.7406e-01,  3.4001e-01,  1.9852e-01, -2.7953e-01,  3.6991e-01,\n",
       "            1.0846e-01,  1.4294e-01,  5.0460e-01, -2.3717e-01,  1.8816e-01,\n",
       "           -6.4986e-01,  7.5006e-02, -2.0631e-01]],\n",
       " \n",
       "         [[ 2.2797e-01, -1.5878e-01, -2.8267e-01, -6.0999e-01, -9.1966e-02,\n",
       "           -1.5180e-01,  1.3710e+00, -5.2778e-01,  1.0381e+00,  3.6600e-02,\n",
       "           -1.5083e+00, -7.5761e-02, -8.8785e-01,  7.7976e-01,  1.1512e+00,\n",
       "            1.4436e-01, -1.9018e+00,  1.7330e+00,  1.0922e-01, -2.5850e-01,\n",
       "            2.1334e-02, -4.8570e-01, -1.4905e-01, -9.7563e-01, -9.9614e-01,\n",
       "            2.3485e-01, -7.1466e-02,  5.3365e-01, -5.6290e-01,  3.7624e-01,\n",
       "           -8.8352e-01,  1.3750e+00, -1.0189e+00, -4.6063e-01,  6.8358e-01,\n",
       "            9.6906e-02,  3.9359e-01, -2.5545e-01,  3.5581e-01, -1.2183e+00,\n",
       "           -2.9422e-01,  1.0097e+00, -1.0951e-01,  1.0004e-01, -6.1969e-01,\n",
       "           -1.3519e-01,  1.2938e+00, -4.6318e-01, -7.4444e-01, -2.8087e-01,\n",
       "           -6.7492e-01, -1.1046e-01,  6.3738e-01, -7.6167e-01, -7.6632e-01,\n",
       "            1.2959e+00,  2.6409e-01,  5.1969e-01,  4.4387e-01, -7.4982e-01,\n",
       "           -1.0839e+00,  5.2939e-01,  5.9532e-01,  7.6892e-01,  2.4456e-01,\n",
       "            4.5289e-03, -3.9375e-01, -7.0120e-01, -2.2724e-01, -4.9459e-01,\n",
       "            6.7994e-01, -2.1749e-01,  7.9523e-01, -3.6484e-01, -2.1871e+00,\n",
       "            2.6787e-01, -6.3438e-01,  3.6179e-01,  8.9995e-01,  4.6231e-01,\n",
       "           -1.9732e+00,  1.1347e+00, -7.7617e-02, -6.5751e-01,  5.0660e-01,\n",
       "           -1.7558e+00, -1.9556e-01, -3.1213e-01, -9.8941e-01, -5.3421e-02,\n",
       "           -3.7063e-01, -1.2385e-01, -3.9090e-01,  8.0257e-01,  4.6604e-01,\n",
       "            1.4670e+00, -8.5652e-01,  7.8876e-01, -2.3971e-01, -4.5958e-02,\n",
       "            4.8537e-01,  1.4916e-01, -9.7878e-02,  8.1166e-02, -6.4773e-02,\n",
       "            4.0616e-01,  3.1894e-01,  8.0167e-02, -5.0389e-01, -7.7582e-01,\n",
       "            4.8504e-01, -3.7845e-01,  5.7100e-01,  1.1993e-01, -6.9160e-01,\n",
       "           -3.3473e-01,  4.4373e-01,  2.0891e-01, -2.8883e-01,  4.2872e-01,\n",
       "            5.2537e-02,  3.4356e-02,  4.4114e-01, -1.2558e-01,  2.2824e-01,\n",
       "           -6.0373e-01,  1.1737e-01, -6.9377e-02]],\n",
       " \n",
       "         [[ 1.9768e-01, -8.8062e-02, -3.2272e-01, -6.9291e-01, -2.5819e-02,\n",
       "           -1.8327e-01,  1.4608e+00, -4.4202e-01,  1.0294e+00,  8.9156e-02,\n",
       "           -1.4123e+00, -1.1479e-01, -1.1067e+00,  7.8760e-01,  1.1409e+00,\n",
       "            1.5253e-01, -1.9442e+00,  1.8712e+00,  2.7645e-01, -2.4968e-01,\n",
       "            1.2303e-02, -5.0725e-01, -2.7664e-01, -1.0976e+00, -1.1133e+00,\n",
       "            3.0255e-01, -3.3549e-01,  5.4023e-01, -4.1991e-01,  6.1701e-01,\n",
       "           -9.0476e-01,  1.1356e+00, -9.2244e-01, -5.9758e-01,  7.6235e-01,\n",
       "           -2.2734e-02,  4.1203e-01, -3.0264e-01,  3.2643e-01, -1.6541e+00,\n",
       "           -3.4497e-01,  1.0139e+00, -1.1699e-01,  6.4422e-02, -7.1565e-01,\n",
       "           -1.8757e-01,  1.5353e+00, -4.7912e-01, -7.0798e-01, -2.2327e-01,\n",
       "           -1.0968e+00, -1.0258e-01,  6.9460e-01, -7.5115e-01, -7.2879e-01,\n",
       "            1.4706e+00,  2.9365e-01,  3.8393e-01,  3.8301e-01, -7.9542e-01,\n",
       "           -1.0397e+00,  6.3834e-01,  5.0631e-01,  7.8021e-01,  2.5019e-01,\n",
       "           -7.4005e-02, -4.8501e-01, -7.0364e-01, -2.3572e-01, -5.4869e-01,\n",
       "            5.1104e-01, -2.4591e-01,  9.1329e-01, -3.5215e-01, -2.3252e+00,\n",
       "            3.7153e-01, -4.7376e-01,  2.2503e-01,  9.8104e-01,  4.3507e-01,\n",
       "           -1.9166e+00,  1.1332e+00, -5.8646e-02, -6.9779e-01,  4.7283e-01,\n",
       "           -1.7532e+00, -1.0488e-01, -4.3388e-01, -1.0869e+00, -1.7181e-01,\n",
       "           -4.5407e-01, -7.1367e-02, -3.7892e-01,  7.2219e-01,  4.9863e-01,\n",
       "            1.6513e+00, -7.8464e-01,  9.3844e-01, -3.4234e-01,  7.1671e-02,\n",
       "            3.9190e-01,  1.7055e-01, -2.1568e-01,  3.2258e-01, -1.3331e-01,\n",
       "            4.3201e-01,  3.1961e-01,  2.6977e-02, -5.1090e-01, -7.9928e-01,\n",
       "            4.6812e-01, -2.9911e-01,  5.4753e-01,  1.0591e-01, -4.5319e-01,\n",
       "           -2.8557e-01,  4.6605e-01,  2.7080e-01, -1.9355e-01,  1.4138e-01,\n",
       "            5.5308e-02,  3.4392e-02,  5.0601e-01,  1.3348e-02,  3.6338e-01,\n",
       "           -4.9505e-01,  5.0810e-02,  1.8993e-02]],\n",
       " \n",
       "         [[ 2.2198e-01, -1.3781e-01, -3.0866e-01, -7.9450e-01,  2.8121e-02,\n",
       "           -1.4428e-01,  1.3492e+00, -4.7286e-01,  9.0384e-01, -8.7804e-04,\n",
       "           -1.2837e+00, -2.2897e-01, -1.1297e+00,  7.8664e-01,  1.0172e+00,\n",
       "            1.6426e-01, -1.7710e+00,  1.9158e+00,  4.0664e-01, -2.4394e-01,\n",
       "            1.2985e-01, -2.6093e-01, -3.2954e-01, -1.2715e+00, -1.0853e+00,\n",
       "            3.0952e-01, -5.9463e-01,  5.9053e-01, -3.5699e-01,  7.8175e-01,\n",
       "           -8.1463e-01,  9.6046e-01, -8.1350e-01, -6.9274e-01,  9.6408e-01,\n",
       "           -9.5299e-02,  2.2450e-01, -3.1616e-01,  2.7955e-01, -1.9378e+00,\n",
       "           -3.7233e-01,  1.0116e+00, -7.2966e-02, -2.7300e-02, -6.8772e-01,\n",
       "           -1.7059e-01,  1.7142e+00, -4.7735e-01, -6.9192e-01, -1.5917e-01,\n",
       "           -1.2731e+00, -7.6041e-02,  8.3484e-01, -7.9502e-01, -7.8060e-01,\n",
       "            1.6210e+00,  2.1168e-01,  2.7468e-01,  3.9892e-01, -7.1330e-01,\n",
       "           -8.8623e-01,  7.2608e-01,  4.7105e-01,  6.9095e-01,  2.6191e-01,\n",
       "           -1.9068e-01, -5.5561e-01, -8.1332e-01, -1.7615e-01, -4.8984e-01,\n",
       "            3.4462e-01, -2.5048e-01,  1.1267e+00, -4.1637e-01, -2.4008e+00,\n",
       "            4.3759e-01, -3.8891e-01,  7.4740e-02,  1.0802e+00,  4.0031e-01,\n",
       "           -1.9156e+00,  1.1326e+00,  8.0314e-02, -6.3356e-01,  3.9104e-01,\n",
       "           -1.6175e+00, -2.7457e-02, -6.9470e-01, -1.2167e+00, -3.7180e-01,\n",
       "           -7.5420e-01,  4.4230e-02, -3.1788e-01,  5.3728e-01,  5.1175e-01,\n",
       "            1.8225e+00, -6.4305e-01,  8.6260e-01, -2.5801e-01,  7.4069e-02,\n",
       "            4.4357e-01,  7.6789e-02, -2.9384e-01,  3.6290e-01, -2.8785e-01,\n",
       "            4.4896e-01,  2.7419e-01, -8.9136e-02, -4.9515e-01, -7.2932e-01,\n",
       "            5.1389e-01, -2.5542e-01,  5.0761e-01,  1.5758e-01, -2.5998e-01,\n",
       "           -2.3911e-01,  4.6500e-01,  2.8432e-01, -2.0849e-01, -4.6139e-02,\n",
       "            2.5901e-02,  3.3292e-02,  6.4988e-01,  2.1521e-01,  5.4214e-01,\n",
       "           -3.3906e-01,  1.0348e-01,  5.8508e-02]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liamc\\AppData\\Local\\Temp\\ipykernel_7628\\1429914352.py:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  poses = torch.Tensor([tup[1].reshape(-1, 128) for tup in pose_emb[int(start_frame) +1:int(end_frame) + 1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([90, 128]),\n",
       " tensor([10, 10, 10, 10, 10, 10,  9,  7,  4,  3,  2,  2,  1,  1,  1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = DataLoader(dataset, collate_fn=collate_fn, batch_size=10)\n",
    "data_pack, labels = next(iter(train))\n",
    "packed_seq = torch.nn.utils.rnn.pack_padded_sequence(data_pack[0], data_pack[1], batch_first=True, enforce_sorted=False)\n",
    "packed_seq.data.shape, packed_seq.batch_sizes\n",
    "\n",
    "#90 is the sum of the true sequence lengths\n",
    "#It aggregates the sequences and batches them along elements of the sequence while keeping track of how many elements are in each batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, layers, num_classes, dropout):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers=layers, batch_first=True,\n",
    "        bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden_dim*2),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim*2, hidden_dim*2),\n",
    "            nn.ReLU(), \n",
    "            nn.BatchNorm1d(hidden_dim*2),\n",
    "            nn.Dropout(p=dropout), \n",
    "            nn.Linear(hidden_dim*2, num_classes)\n",
    "\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x_pack):\n",
    "        x, x_len = x_pack\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
    "        encoded_x = self.gru(packed_x)[0]\n",
    "        unpacked_encoded_x, lens = nn.utils.rnn.pad_packed_sequence(encoded_x, batch_first=True)\n",
    "        out = F.max_pool1d(unpacked_encoded_x.permute(0, 2, 1), unpacked_encoded_x.shape[1]).squeeze(2)\n",
    "        decoded_x = self.fc(out)\n",
    "        \n",
    "        return F.softmax(decoded_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model parameters\n",
    "hidden_size = 200\n",
    "input_size = 128\n",
    "num_layers = 50\n",
    "num_classes = 4\n",
    "dropout =.1\n",
    "model = GRUNet(input_size, hidden_size, num_layers, num_classes, dropout)\n",
    "#training parameters\n",
    "train_dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=10)\n",
    "validation_dataloader = None\n",
    "N_EPOCH = 40\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_loss_and_correct(model, batch, criterion, device):\n",
    "    data, target = batch\n",
    "    target = target.long().to(device)\n",
    "    data = (data[0].to(device), data[1])\n",
    "    pred = model(data)\n",
    "    classes = pred.max(dim = 1)[1]\n",
    "    loss = criterion(pred, target)\n",
    "    total = len(target) \n",
    "    correct = (classes == target).sum()\n",
    "    size = total\n",
    "    return loss, correct, size\n",
    "\n",
    "\n",
    "def step(loss, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: carpfire. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\liamc\\Desktop\\fencing_vision\\wandb\\run-20220504_151319-yg2l7mdf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/carpfire/test-project/runs/yg2l7mdf\" target=\"_blank\">stellar-tie-fighter-1</a></strong> to <a href=\"https://wandb.ai/carpfire/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liamc\\AppData\\Local\\Temp\\ipykernel_7628\\64609166.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(decoded_x)\n",
      "Train Loss 1.30840060540608, Train Acc 0.42500001032437595: 100%|██████████| 40/40 [03:20<00:00,  5.02s/it]  \n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_accuracies = []\n",
    "validation_losses = []\n",
    "validation_accuracies = []\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "pbar = tqdm(range(N_EPOCH))\n",
    "\n",
    "def refresh_loss(l, a):\n",
    "    pbar.set_description(f'Train Loss {l}, Train Acc {a}')\n",
    "    pbar.refresh()\n",
    "\n",
    "wandb.init(project=\"test-project\", entity='carpfire')\n",
    "\n",
    "wandb.config = {\n",
    "    'learning_rate':.001,\n",
    "    'epochs':N_EPOCH,\n",
    "    'batch_size':10\n",
    "}\n",
    "\n",
    "\n",
    "for e in pbar:\n",
    "    total_train_loss = 0.0\n",
    "    train_correct = []\n",
    "    total_validation_loss = 0.0\n",
    "    validation_correct = []\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        loss, correct, size = get_loss_and_correct(model, batch, criterion, device)\n",
    "        step(loss, optimizer)\n",
    "        total_train_loss += loss.item()\n",
    "        train_correct.append((correct/size).item())\n",
    "    # with torch.no_grad():\n",
    "    #     for batch in validation_dataloader:\n",
    "    #         loss, correct, size = get_loss_and_correct(model, batch, criterion, device)\n",
    "    #         total_validation_loss += loss.item()\n",
    "    #         validation_correct.append((correct/size).item())\n",
    "\n",
    "    train_len = len(train_correct)\n",
    "    #val_len = len(validation_correct)    \n",
    "    mean_train_loss = total_train_loss / (len(train_dataloader))\n",
    "    train_accuracy = sum(train_correct)/train_len\n",
    "    refresh_loss(mean_train_loss, train_accuracy)\n",
    "    wandb.log({\"loss\":mean_train_loss})\n",
    "    #mean_validation_loss = total_validation_loss / (len(test_ds))\n",
    "    #validation_accuracy = sum(validation_correct) /val_len\n",
    "    train_losses.append(mean_train_loss)\n",
    "    #validation_losses.append(mean_validation_loss)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    # validation_accuracies.append(validation_accuracy)\n",
    "    wandb.watch(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimal Example with the data loaders set. What do I want know that I didn't have before.\n",
    "\n",
    "- Ease of tweaking, experimentation \n",
    "    - Modeling:\n",
    "        - Easy to experiment with different models/hyperparameters\n",
    "    - Data Loading\n",
    "        - Easy Refinement and One Shot Design \n",
    "- Easy Documentation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2272d772725abaa0e9a8171d57b4be3848b3cc5e0dc324a7986cfa1a76fee20b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('fencing_viz')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
